{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## Nicholas Schenone - A13599911"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 trials\n",
    "- 7 classifiers\n",
    "    - SVM\n",
    "    - Logistic Regression\n",
    "    - Decision Tree\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "    - KNN\n",
    "    - Random Forest\n",
    "- 3 datasets\n",
    "    - Heart Disease: https://www.kaggle.com/ronitf/heart-disease-uci\n",
    "    - Mushroom: https://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "    - Adult Data Set: https://archive.ics.uci.edu/ml/datasets/Adult\n",
    "- 2 partitions (20/80, 80/20)\n",
    "- 2 accuracies per (train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import json\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adult_pre_process(data_path=\"data/adult/adult.csv\", split=0.2):\n",
    "    df_adult = pd.read_csv(data_path)\n",
    "    df_adult_one_hot = pd.get_dummies(df_adult);\n",
    "    \n",
    "    X = df_adult_one_hot.iloc[:,0 : len(df_adult_one_hot.columns) - 1]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    y = df_adult_one_hot.iloc[:, len(df_adult_one_hot.columns) - 1]\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split)\n",
    "\n",
    "    return X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "def heart_pre_process(data_path=\"data/heart_disease/heart.csv\", split=0.2):\n",
    "    df_heart = pd.read_csv(data_path)\n",
    "    X = df_heart.iloc[:, 0 : len(df_heart.columns) - 1]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    y = df_heart.iloc[:, len(df_heart.columns) - 1]\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split)\n",
    "\n",
    "    return X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "def mushroom_pre_process(data_path=\"data/mushroom/mushroom.csv\", split=0.2):\n",
    "    df_mushroom = pd.read_csv(data_path, header=None)\n",
    "    df_mush_one_hot = pd.get_dummies(df_mushroom);\n",
    "    \n",
    "    X = df_mush_one_hot.iloc[:,1:]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    y = df_mush_one_hot.iloc[:, :1]\n",
    "    y = y.values.ravel()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split)\n",
    "\n",
    "    return X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "def pre_process(dataset, split=0.2):\n",
    "    if dataset == \"happy\":\n",
    "        return happiness_pre_process(split=split)\n",
    "    elif dataset == \"mush\":\n",
    "        return mushroom_pre_process(split=split)\n",
    "    elif dataset == \"heart\":\n",
    "        return heart_pre_process(split=split)\n",
    "    elif dataset == \"adult\":\n",
    "        return adult_pre_process(split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_X, heart_y, heart_X_train, heart_X_test, heart_y_train, heart_y_test = heart_pre_process(split=0.2)\n",
    "\n",
    "mush_X, mush_y, mush_X_train, mush_X_test, mush_y_train, mush_y_test = mushroom_pre_process(split=0.2)\n",
    "\n",
    "adult_X, adult_y, adult_X_train, adult_X_test, adult_y_train, adult_y_test = adult_pre_process(split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "def clf_SVM(param_grid):\n",
    "    return svm.SVC(C = param_grid[\"C\"],\n",
    "                   gamma=param_grid[\"gamma\"],\n",
    "                   kernel=param_grid[\"kernel\"],\n",
    "                   max_iter = 10000)\n",
    "\n",
    "# Logistic Regression\n",
    "def clf_log(param_grid):\n",
    "    return LogisticRegression(C = param_grid[\"C\"],\n",
    "                              penalty = param_grid[\"penalty\"],\n",
    "                              solver=\"liblinear\",\n",
    "                              max_iter = 10000)\n",
    "\n",
    "# Decision Tree\n",
    "def clf_tree(param_grid):\n",
    "    return DecisionTreeClassifier(criterion=param_grid[\"criterion\"],\n",
    "                                  max_depth=param_grid[\"max_depth\"])\n",
    "\n",
    "# Perceptron\n",
    "def clf_perc(param_grid):\n",
    "    return Perceptron(penalty=param_grid[\"penalty\"],\n",
    "                      alpha=param_grid[\"alpha\"],\n",
    "                      max_iter=param_grid[\"max_iter\"],\n",
    "                      tol=param_grid[\"tol\"],\n",
    "                      early_stopping=param_grid[\"early_stopping\"])\n",
    "\n",
    "# Multi-Layer Perceptron\n",
    "def clf_mlp(param_grid):\n",
    "    return MLPClassifier(activation=param_grid[\"activation\"],\n",
    "                      solver=param_grid[\"solver\"],\n",
    "                      hidden_layer_sizes=param_grid[\"hidden_layer_sizes\"],\n",
    "                      max_iter=param_grid[\"max_iter\"],\n",
    "                      tol=param_grid[\"tol\"],\n",
    "                      early_stopping=param_grid[\"early_stopping\"])\n",
    "\n",
    "# KNN\n",
    "def clf_knn(param_grid):\n",
    "    return KNeighborsClassifier(n_neighbors=param_grid[\"n_neighbors\"])\n",
    "\n",
    "# Random Forest\n",
    "def clf_rf(param_grid):\n",
    "    return RandomForestClassifier(bootstrap=param_grid[\"bootstrap\"],\n",
    "                                 max_depth=param_grid[\"max_depth\"],\n",
    "                                 max_features=param_grid[\"max_features\"],\n",
    "                                 min_samples_leaf=param_grid[\"min_samples_leaf\"],\n",
    "                                 min_samples_split=param_grid[\"min_samples_split\"],\n",
    "                                 n_estimators=param_grid[\"n_estimators\"])\n",
    "\n",
    "# General\n",
    "def clf(model, param_grid):\n",
    "    if model == \"svm\":\n",
    "        return clf_SVM(param_grid)\n",
    "    elif model==\"log\":\n",
    "        return clf_log(param_grid)\n",
    "    elif model==\"tree\":\n",
    "        return clf_tree(param_grid)\n",
    "    elif model==\"perc\":\n",
    "        return clf_perc(param_grid)\n",
    "    elif model==\"mlp\":\n",
    "        return clf_mlp(param_grid)\n",
    "    elif model==\"knn\":\n",
    "        return clf_knn(param_grid)\n",
    "    elif model==\"rf\":\n",
    "        return clf_rf(param_grid)\n",
    "    \n",
    "def train_model(classifier, X_train, y_train):\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "def hyper_tune(X_train, y_train, estimator, param_grid, k_top=3):\n",
    "    grid_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, cv=10, n_iter=20, n_jobs=-1, verbose=10)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    results.sort_values(by='rank_test_score', inplace=True)\n",
    "    out = []\n",
    "    [out.append(results.loc[i, 'params']) for i in range(k_top)]\n",
    "    print(f\"Best {k_top} params:\", out)\n",
    "    return out\n",
    "\n",
    "def evalModel(classifer, X_test, y_test):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    accuracy= accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    f_score = f1_score(y_test, y_pred, average=\"macro\") \n",
    "    \n",
    "    return (accuracy, precision, recall, f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_param_grid = {\n",
    "    \"C\" : [1, 10, 100, 1000, 10000],\n",
    "    \"gamma\" : [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"kernel\" : [\"linear\", \"rbf\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mushroom SVM Tuning\n",
    "best_param_grid_mush = hyper_tune(mush_X_train, mush_y_train, svm.SVC(), svm_param_grid)\n",
    "\n",
    "with open('params/svm/best_param_grid_mush', 'w') as f:\n",
    "    json.dump(best_param_grid_mush, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart SVM Tuning\n",
    "best_param_grid_heart = hyper_tune(heart_X_train, heart_y_train, svm.SVC(), svm_param_grid)\n",
    "\n",
    "with open('params/svm/best_param_grid_heart', 'w') as f:\n",
    "    json.dump(best_param_grid_heart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult SVM Tuning\n",
    "best_param_grid_adult = hyper_tune(adult_X_train, adult_y_train, svm.SVC(), svm_param_grid)\n",
    "\n",
    "with open('params/svm/best_param_grid_adult', 'w') as f:\n",
    "    json.dump(best_param_grid_adult, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_param_grid = {\n",
    "    \"C\" : [1, 10, 100, 1000, 10000],\n",
    "    \"penalty\" : [\"l1\", \"l2\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mushroom Logistic Regression Tuning\n",
    "best_param_grid_mush = hyper_tune(mush_X_train, mush_y_train, LogisticRegression(), log_param_grid)\n",
    "\n",
    "with open('params/log/best_param_grid_mush', 'w') as f:\n",
    "    json.dump(best_param_grid_mush, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Logistic Regression Tuning\n",
    "best_param_grid_heart = hyper_tune(heart_X_train, heart_y_train, LogisticRegression(), log_param_grid)\n",
    "\n",
    "with open('params/log/best_param_grid_heart', 'w') as f:\n",
    "    json.dump(best_param_grid_heart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult Logistic Regression Tuning\n",
    "best_param_grid_adult = hyper_tune(adult_X_train, adult_y_train, LogisticRegression(), log_param_grid)\n",
    "\n",
    "with open('params/log/best_param_grid_adult', 'w') as f:\n",
    "    json.dump(best_param_grid_adult, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_param_grid = {\n",
    "    \"criterion\" : ['gini', 'entropy'],\n",
    "    \"max_depth\" : [4,6,8,12],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mushroom Decision Tree Tuning\n",
    "best_param_grid_mush = hyper_tune(mush_X_train, mush_y_train, DecisionTreeClassifier(), tree_param_grid)\n",
    "\n",
    "with open('params/tree/best_param_grid_mush', 'w') as f:\n",
    "    json.dump(best_param_grid_mush, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Decision Tree Tuning\n",
    "best_param_grid_heart = hyper_tune(heart_X_train, heart_y_train, DecisionTreeClassifier(), tree_param_grid)\n",
    "\n",
    "with open('params/tree/best_param_grid_heart', 'w') as f:\n",
    "    json.dump(best_param_grid_heart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult Decision Tree Tuning\n",
    "best_param_grid_adult = hyper_tune(adult_X_train, adult_y_train, DecisionTreeClassifier(), tree_param_grid)\n",
    "\n",
    "with open('params/tree/best_param_grid_adult', 'w') as f:\n",
    "    json.dump(best_param_grid_adult, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_param_grid = {\n",
    "    \"penalty\" : [None, \"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"alpha\" : [0.001, 0.0001, 0.00001],\n",
    "    \"max_iter\" : [500, 1000, 2000],\n",
    "    \"tol\" : [1e-4, 1e-3, 1e-2],\n",
    "    \"early_stopping\" : [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mushroom Perceptron Tuning\n",
    "best_param_grid_mush = hyper_tune(mush_X_train, mush_y_train, Perceptron(), perc_param_grid)\n",
    "\n",
    "with open('params/perc/best_param_grid_mush', 'w') as f:\n",
    "    json.dump(best_param_grid_mush, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Perceptron Tuning\n",
    "best_param_grid_heart = hyper_tune(heart_X_train, heart_y_train, Perceptron(), perc_param_grid)\n",
    "\n",
    "with open('params/perc/best_param_grid_heart', 'w') as f:\n",
    "    json.dump(best_param_grid_heart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult Perceptron Tuning\n",
    "best_param_grid_adult = hyper_tune(adult_X_train, adult_y_train, Perceptron(), perc_param_grid)\n",
    "\n",
    "with open('params/perc/best_param_grid_adult', 'w') as f:\n",
    "    json.dump(best_param_grid_adult, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_param_grid = {\n",
    "    \"hidden_layer_sizes\" : [(100,), (50,), (200,), (25,)],\n",
    "    \"activation\" : [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "    \"solver\" : [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "    \"max_iter\" : [200, 100, 300],\n",
    "    \"tol\" : [1e-4, 1e-3, 1e-5],\n",
    "    \"early_stopping\" : [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mushroom Perceptron Tuning\n",
    "best_param_grid_mush = hyper_tune(mush_X_train, mush_y_train, MLPClassifier(), mlp_param_grid)\n",
    "\n",
    "with open('params/mlp/best_param_grid_mush', 'w') as f:\n",
    "    json.dump(best_param_grid_mush, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Perceptron Tuning\n",
    "best_param_grid_heart = hyper_tune(heart_X_train, heart_y_train, MLPClassifier(), mlp_param_grid)\n",
    "\n",
    "with open('params/mlp/best_param_grid_heart', 'w') as f:\n",
    "    json.dump(best_param_grid_heart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult Perceptron Tuning\n",
    "best_param_grid_adult = hyper_tune(adult_X_train, adult_y_train, MLPClassifier(), mlp_param_grid)\n",
    "\n",
    "with open('params/mlp/best_param_grid_adult', 'w') as f:\n",
    "    json.dump(best_param_grid_adult, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    \"n_neighbors\" : [1, 3, 5, 9, 15, 25, 50, 75, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mushroom Decision Tree Tuning\n",
    "best_param_grid_mush = hyper_tune(mush_X_train, mush_y_train, KNeighborsClassifier(), knn_param_grid)\n",
    "\n",
    "with open('params/knn/best_param_grid_mush', 'w') as f:\n",
    "    json.dump(best_param_grid_mush, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Decision Tree Tuning\n",
    "best_param_grid_heart = hyper_tune(heart_X_train, heart_y_train, KNeighborsClassifier(), knn_param_grid)\n",
    "\n",
    "with open('params/knn/best_param_grid_heart', 'w') as f:\n",
    "    json.dump(best_param_grid_heart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult Decision Tree Tuning\n",
    "best_param_grid_adult = hyper_tune(adult_X_train, adult_y_train, KNeighborsClassifier(), knn_param_grid)\n",
    "\n",
    "with open('params/knn/best_param_grid_adult', 'w') as f:\n",
    "    json.dump(best_param_grid_adult, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mushroom Decision Tree Tuning\n",
    "best_param_grid_mush = hyper_tune(mush_X_train, mush_y_train, RandomForestClassifier(), rf_param_grid)\n",
    "\n",
    "with open('params/rf/best_param_grid_mush', 'w') as f:\n",
    "    json.dump(best_param_grid_mush, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Decision Tree Tuning\n",
    "best_param_grid_heart = hyper_tune(heart_X_train, heart_y_train, RandomForestClassifier(), rf_param_grid)\n",
    "\n",
    "with open('params/rf/best_param_grid_heart', 'w') as f:\n",
    "    json.dump(best_param_grid_heart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult Decision Tree Tuning\n",
    "best_param_grid_adult = hyper_tune(adult_X_train, adult_y_train, RandomForestClassifier(), rf_param_grid)\n",
    "\n",
    "with open('params/rf/best_param_grid_adult', 'w') as f:\n",
    "    json.dump(best_param_grid_adult, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 trials\n",
    "    - 3 datasets\n",
    "        - 7 models\n",
    "            - 2 splits (80/20, 20/80)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 3\n",
    "datasets = ['adult', 'mush', 'heart']\n",
    "splits = [0.2, 0.8]\n",
    "models = ['log', 'svm', 'tree', 'perc', 'mlp', 'knn', 'rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through datasets\n",
    "dataset_scores = {}\n",
    "for dataset in datasets:\n",
    "    \n",
    "    # Loop through dataset splits\n",
    "    split_scores = {}\n",
    "    for split in splits:\n",
    "\n",
    "        # Prepare data splits\n",
    "        X, y, X_train, X_test, y_train, y_test = pre_process(dataset=dataset, split=split)\n",
    "        \n",
    "        # Loop through trials\n",
    "        trial_scores = {}\n",
    "        for i in range(num_trials):\n",
    "            \n",
    "            # Loop through models\n",
    "            model_scores = {}\n",
    "            for model in models:\n",
    "                \n",
    "                # Load best model params for given model and dataset\n",
    "                with open(f'params/{model}/best_param_grid_{dataset}', 'r') as f:\n",
    "                    best_param_grid = json.load(f)\n",
    "    \n",
    "                    # Create classifier\n",
    "                    classifier = clf(model=model, param_grid=best_param_grid[i])\n",
    "\n",
    "                    # Train classifier\n",
    "                    print(f\"Training {dataset}-{split}-{i}-{model}\")\n",
    "                    train_model(classifier, X_train, y_train)\n",
    "\n",
    "                    # Evaluate classifier\n",
    "                    print(f\"Evaluating {dataset}-{split}-{i}-{model}\")\n",
    "                    acc, prec, rec, f = evalModel(classifier, X_test, y_test)\n",
    "                    test = {\"accuracy\" : acc, \"precision\": prec, \"recall\" : rec, \"f1_score\" : f}  \n",
    "                    acc, prec, rec, f = evalModel(classifier, X_train, y_train)\n",
    "                    train = {\"accuracy\" : acc, \"precision\": prec, \"recall\" : rec, \"f1_score\" : f} \n",
    "                    \n",
    "                    classifier_eval = {\"train\" : train, \"test\" : test}\n",
    "                    \n",
    "                # Add evaluation scores for given model\n",
    "                model_scores.update({f\"model_{model}\" : classifier_eval})\n",
    "                \n",
    "            # Add model scores for given trial\n",
    "            trial_scores.update({f\"trial_{i}\" : model_scores})\n",
    "\n",
    "        # Add trial scores for given model\n",
    "        split_scores.update({f\"split_{split}\": trial_scores})\n",
    "\n",
    "    # Add split scores for given dataset\n",
    "    dataset_scores.update({f\"data_{dataset}\": split_scores})\n",
    "    \n",
    "with open('scores/dataset_scores', 'w') as f:\n",
    "    json.dump(dataset_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores/dataset_scores', 'r') as f:\n",
    "    dataset_scores = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_table_acc(_split_name, _model_name, dataset_scores=dataset_scores, models=models):\n",
    "    # Init empty lists to gather data\n",
    "    adult_train_acc = []\n",
    "    adult_test_acc = []\n",
    "    adult_train_f1 = []\n",
    "    adult_test_f1 = []\n",
    "    \n",
    "    heart_train_acc = []\n",
    "    heart_test_acc = []\n",
    "    heart_train_f1 = []\n",
    "    heart_test_f1 = []\n",
    "    \n",
    "    mush_train_acc = []\n",
    "    mush_test_acc = []\n",
    "    mush_train_f1 = []\n",
    "    mush_test_f1 = []\n",
    "    \n",
    "    # Loop through all datasets, trials, splits, etc and add data\n",
    "    # to empty lists\n",
    "    for dataset_name, dataset_data in dataset_scores.items():\n",
    "        for trial_name, trial_data in dataset_data[_split_name].items():\n",
    "            if dataset_name == \"data_adult\":\n",
    "                adult_train_acc.append(trial_data[_model_name][\"train\"][\"accuracy\"])\n",
    "                adult_test_acc.append(trial_data[_model_name][\"test\"][\"accuracy\"])\n",
    "                adult_train_f1.append(trial_data[_model_name][\"train\"][\"f1_score\"])\n",
    "                adult_test_f1.append(trial_data[_model_name][\"test\"][\"f1_score\"])\n",
    "            elif dataset_name == \"data_mush\":\n",
    "                mush_train_acc.append(trial_data[_model_name][\"train\"][\"accuracy\"])\n",
    "                mush_test_acc.append(trial_data[_model_name][\"test\"][\"accuracy\"])\n",
    "                mush_train_f1.append(trial_data[_model_name][\"train\"][\"f1_score\"])\n",
    "                mush_test_f1.append(trial_data[_model_name][\"test\"][\"f1_score\"])\n",
    "            elif dataset_name == \"data_heart\":\n",
    "                heart_train_acc.append(trial_data[_model_name][\"train\"][\"accuracy\"])\n",
    "                heart_test_acc.append(trial_data[_model_name][\"test\"][\"accuracy\"])\n",
    "                heart_train_f1.append(trial_data[_model_name][\"train\"][\"f1_score\"])\n",
    "                heart_test_f1.append(trial_data[_model_name][\"test\"][\"f1_score\"])\n",
    "    \n",
    "    # Convert lists to numpy arrays for computations (mean, std)\n",
    "    adult_train_acc = np.asarray(adult_train_acc)\n",
    "    adult_test_acc = np.asarray(adult_test_acc)\n",
    "    adult_train_f1 = np.asarray(adult_train_f1)\n",
    "    adult_test_f1 = np.asarray(adult_test_f1)\n",
    "    \n",
    "    heart_train_acc = np.asarray(heart_train_acc)\n",
    "    heart_test_acc = np.asarray(heart_test_acc)\n",
    "    heart_train_f1 = np.asarray(heart_train_f1)\n",
    "    heart_test_f1 = np.asarray(heart_test_f1)\n",
    "    \n",
    "    mush_train_acc = np.asarray(mush_train_acc)\n",
    "    mush_test_acc = np.asarray(mush_test_acc)\n",
    "    mush_train_f1 = np.asarray(mush_train_f1)\n",
    "    mush_test_f1 = np.asarray(mush_test_f1)\n",
    "    \n",
    "    # Display variable dictionaries\n",
    "    disp_split = {\n",
    "        \"split_0.2\" : \"80/20\",\n",
    "        \"split_0.8\" : \"20/80\"\n",
    "    }\n",
    "    \n",
    "    disp_model = {\n",
    "        'model_log': \"Logarithmic Regression\",\n",
    "        'model_svm': \"Support Vector Machine\",\n",
    "        'model_tree': \"Decision Tree\",\n",
    "        'model_perc': \"Perceptron\",\n",
    "        'model_mlp': \"Multi-Layer Perceptron\",\n",
    "        'model_knn': \"K-Nearest Neighbor\",\n",
    "        'model_rf': \"Random Forest\"\n",
    "    }\n",
    "    \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame([[\n",
    "        f\"{np.around(100 * adult_train_acc.mean(), decimals=2)} ± {np.around(100 * adult_train_acc.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * adult_test_acc.mean(), decimals=2)} ± {np.around(100 * adult_test_acc.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * mush_train_acc.mean(), decimals=2)} ± {np.around(100 * mush_train_acc.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * mush_test_acc.mean(), decimals=2)} ± {np.around(100 * mush_test_acc.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * heart_train_acc.mean(), decimals=2)} ± {np.around(100 * heart_train_acc.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * heart_test_acc.mean(), decimals=2)} ± {np.around(100 * heart_test_acc.std(), decimals=2)}%\",\n",
    "    ]], columns=[f\"Adult Train Acc {disp_split[_split_name]}\",\n",
    "                 f\"Adult Test Acc {disp_split[_split_name]}\",\n",
    "                 f\"Mushroom Train Acc {disp_split[_split_name]}\",\n",
    "                 f\"Mushroom Test Acc {disp_split[_split_name]}\",\n",
    "                 f\"Heart Train Acc {disp_split[_split_name]}\",\n",
    "                 f\"Heart Test Acc {disp_split[_split_name]}\"])\n",
    "    df[f\"Avg Train Acc {disp_split[_split_name]}\"] = f\"{np.around(100 * np.array([adult_train_acc.mean(), mush_train_acc.mean(), heart_train_acc.mean()]).mean(), decimals=2)}%\"\n",
    "    \n",
    "    df[f\"Avg Test Acc {disp_split[_split_name]}\"] = f\"{np.around(100 * np.array([adult_test_acc.mean(), mush_test_acc.mean(), heart_test_acc.mean()]).mean(), decimals=2)}%\"\n",
    "    df = df.T\n",
    "    df.columns = [f\"{disp_model[_model_name]}\"]\n",
    "    return df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_table_f1(_split_name, _model_name, dataset_scores=dataset_scores, models=models):\n",
    "    # Init empty lists to gather data\n",
    "    adult_train_acc = []\n",
    "    adult_test_acc = []\n",
    "    adult_train_f1 = []\n",
    "    adult_test_f1 = []\n",
    "    \n",
    "    heart_train_acc = []\n",
    "    heart_test_acc = []\n",
    "    heart_train_f1 = []\n",
    "    heart_test_f1 = []\n",
    "    \n",
    "    mush_train_acc = []\n",
    "    mush_test_acc = []\n",
    "    mush_train_f1 = []\n",
    "    mush_test_f1 = []\n",
    "    \n",
    "    # Loop through all datasets, trials, splits, etc and add data\n",
    "    # to empty lists\n",
    "    for dataset_name, dataset_data in dataset_scores.items():\n",
    "        for trial_name, trial_data in dataset_data[_split_name].items():\n",
    "            if dataset_name == \"data_adult\":\n",
    "                adult_train_acc.append(trial_data[_model_name][\"train\"][\"accuracy\"])\n",
    "                adult_test_acc.append(trial_data[_model_name][\"test\"][\"accuracy\"])\n",
    "                adult_train_f1.append(trial_data[_model_name][\"train\"][\"f1_score\"])\n",
    "                adult_test_f1.append(trial_data[_model_name][\"test\"][\"f1_score\"])\n",
    "            elif dataset_name == \"data_mush\":\n",
    "                mush_train_acc.append(trial_data[_model_name][\"train\"][\"accuracy\"])\n",
    "                mush_test_acc.append(trial_data[_model_name][\"test\"][\"accuracy\"])\n",
    "                mush_train_f1.append(trial_data[_model_name][\"train\"][\"f1_score\"])\n",
    "                mush_test_f1.append(trial_data[_model_name][\"test\"][\"f1_score\"])\n",
    "            elif dataset_name == \"data_heart\":\n",
    "                heart_train_acc.append(trial_data[_model_name][\"train\"][\"accuracy\"])\n",
    "                heart_test_acc.append(trial_data[_model_name][\"test\"][\"accuracy\"])\n",
    "                heart_train_f1.append(trial_data[_model_name][\"train\"][\"f1_score\"])\n",
    "                heart_test_f1.append(trial_data[_model_name][\"test\"][\"f1_score\"])\n",
    "    \n",
    "    # Convert lists to numpy arrays for computations (mean, std)\n",
    "    adult_train_acc = np.asarray(adult_train_acc)\n",
    "    adult_test_acc = np.asarray(adult_test_acc)\n",
    "    adult_train_f1 = np.asarray(adult_train_f1)\n",
    "    adult_test_f1 = np.asarray(adult_test_f1)\n",
    "    \n",
    "    heart_train_acc = np.asarray(heart_train_acc)\n",
    "    heart_test_acc = np.asarray(heart_test_acc)\n",
    "    heart_train_f1 = np.asarray(heart_train_f1)\n",
    "    heart_test_f1 = np.asarray(heart_test_f1)\n",
    "    \n",
    "    mush_train_acc = np.asarray(mush_train_acc)\n",
    "    mush_test_acc = np.asarray(mush_test_acc)\n",
    "    mush_train_f1 = np.asarray(mush_train_f1)\n",
    "    mush_test_f1 = np.asarray(mush_test_f1)\n",
    "    \n",
    "    # Display variable dictionaries\n",
    "    disp_split = {\n",
    "        \"split_0.2\" : \"80/20\",\n",
    "        \"split_0.8\" : \"20/80\"\n",
    "    }\n",
    "    \n",
    "    disp_model = {\n",
    "        'model_log': \"Logarithmic Regression\",\n",
    "        'model_svm': \"Support Vector Machine\",\n",
    "        'model_tree': \"Decision Tree\",\n",
    "        'model_perc': \"Perceptron\",\n",
    "        'model_mlp': \"Multi-Layer Perceptron\",\n",
    "        'model_knn': \"K-Nearest Neighbor\",\n",
    "        'model_rf': \"Random Forest\"\n",
    "    }\n",
    "    \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame([[\n",
    "        f\"{np.around(100 * adult_train_f1.mean(), decimals=2)} ± {np.around(100 * adult_train_f1.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * adult_test_f1.mean(), decimals=2)} ± {np.around(100 * adult_test_f1.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * mush_train_f1.mean(), decimals=2)} ± {np.around(100 * mush_train_f1.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * mush_test_f1.mean(), decimals=2)} ± {np.around(100 * mush_test_f1.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * heart_train_f1.mean(), decimals=2)} ± {np.around(100 * heart_train_f1.std(), decimals=2)}%\",\n",
    "        f\"{np.around(100 * heart_test_f1.mean(), decimals=2)} ± {np.around(100 * heart_test_f1.std(), decimals=2)}%\",\n",
    "    ]], columns=[f\"Adult Train F1 {disp_split[_split_name]}\",\n",
    "                 f\"Adult Test F1 {disp_split[_split_name]}\",\n",
    "                 f\"Mushroom Train F1 {disp_split[_split_name]}\",\n",
    "                 f\"Mushroom Test F1 {disp_split[_split_name]}\",\n",
    "                 f\"Heart Train F1 {disp_split[_split_name]}\",\n",
    "                 f\"Heart Test F1 {disp_split[_split_name]}\"])\n",
    "    df[f\"Avg Train F1 {disp_split[_split_name]}\"] = f\"{np.around(100 * np.array([adult_train_f1.mean(), mush_train_f1.mean(), heart_train_f1.mean()]).mean(), decimals=2)}%\"\n",
    "    \n",
    "    df[f\"Avg Test F1 {disp_split[_split_name]}\"] = f\"{np.around(100 * np.array([adult_test_f1.mean(), mush_test_f1.mean(), heart_test_f1.mean()]).mean(), decimals=2)}%\"\n",
    "    df = df.T\n",
    "    df.columns = [f\"{disp_model[_model_name]}\"]\n",
    "    return df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['model_log', 'model_svm', 'model_tree', 'model_perc', 'model_mlp', 'model_knn','model_rf']\n",
    "data_tables = []\n",
    "for model in model_list:\n",
    "    data_tables.append(data_table_acc(\"split_0.2\", model))\n",
    "# data_table_split_02_acc = pd.concat(data_tables).sort_values(by=['Avg Test Acc 80/20'], ascending=False)\n",
    "data_table_split_02_acc = pd.concat(data_tables)\n",
    "data_table_split_02_acc#.to_csv(\"tables/data_table_split_02_acc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['model_log', 'model_svm', 'model_tree', 'model_perc', 'model_mlp', 'model_knn','model_rf']\n",
    "data_tables = []\n",
    "for model in model_list:\n",
    "    data_tables.append(data_table_f1(\"split_0.2\", model))\n",
    "# data_table_split_02_f1 = pd.concat(data_tables).sort_values(by=['Avg Test F1 80/20'], ascending=False)\n",
    "data_table_split_02_f1 = pd.concat(data_tables)\n",
    "data_table_split_02_f1#.to_csv(\"tables/data_table_split_02_f1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20/80 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['model_log', 'model_svm', 'model_tree', 'model_perc', 'model_mlp', 'model_knn','model_rf']\n",
    "data_tables = []\n",
    "for model in model_list:\n",
    "    data_tables.append(data_table_acc(\"split_0.8\", model))\n",
    "# data_table_split_08_acc = pd.concat(data_tables).sort_values(by=['Avg Test Acc 20/80'], ascending=False)\n",
    "data_table_split_08_acc = pd.concat(data_tables)\n",
    "data_table_split_08_acc#.to_csv(\"tables/data_table_split_08_acc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['model_log', 'model_svm', 'model_tree', 'model_perc', 'model_mlp', 'model_knn','model_rf']\n",
    "data_tables = []\n",
    "for model in model_list:\n",
    "    data_tables.append(data_table_f1(\"split_0.8\", model))\n",
    "# data_table_split_08_f1 = pd.concat(data_tables).sort_values(by=['Avg Test F1 20/80'], ascending=False)\n",
    "data_table_split_08_f1 = pd.concat(data_tables)\n",
    "data_table_split_08_f1\n",
    "data_table_split_08_f1#.to_csv(\"tables/data_table_split_08_f1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranked Avg Test Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_02 = data_table_split_02_acc[\"Avg Test Acc 80/20\"]\n",
    "acc_08 = data_table_split_08_acc[\"Avg Test Acc 20/80\"]\n",
    "df_acc = pd.concat((acc_02, acc_08), axis=1)\n",
    "df_acc['Avg Test Acc 80/20'].replace(regex=True,inplace=True,to_replace=r'%',value=r'')\n",
    "df_acc['Avg Test Acc 20/80'].replace(regex=True,inplace=True,to_replace=r'%',value=r'')\n",
    "df_acc['Avg Test Acc'] = df_acc.astype('float64').mean(axis=1).to_frame()\n",
    "df_acc.sort_values(by=['Avg Test Acc'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranked Avg F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_02 = data_table_split_02_f1[\"Avg Test F1 80/20\"]\n",
    "f1_08 = data_table_split_08_f1[\"Avg Test F1 20/80\"]\n",
    "df_f1 = pd.concat((f1_02, f1_08), axis=1)\n",
    "df_f1['Avg Test F1 80/20'].replace(regex=True,inplace=True,to_replace=r'%',value=r'')\n",
    "df_f1['Avg Test F1 20/80'].replace(regex=True,inplace=True,to_replace=r'%',value=r'')\n",
    "df_f1['Avg Test F1'] = df_f1.astype('float64').mean(axis=1).to_frame()\n",
    "df_f1.sort_values(by=['Avg Test F1'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
